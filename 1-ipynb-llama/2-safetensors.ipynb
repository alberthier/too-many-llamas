{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# safetensors conversion\n",
    "\n",
    "Llama tensors are serialized in `safetensors` format. For Llama 3 1B model, the tensors are encoded in `bfloat16`.\n",
    "\n",
    "`bfloat16` cannot be used on CPU, we need to convert them to `float32`. `bfloat16`s are just tuncated `float32`s : the lower bits of the exponent are truncated. We will simply pad each `bfloat16` with two `0` bytes and cast the result to `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_files = [\n",
    "    \"https://huggingface.co/meta-llama/Llama-3.2-1B/resolve/main/model.safetensors\"\n",
    "]\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "for required_file in required_files:\n",
    "    local_file = required_file.split(\"/\")[-1]\n",
    "    if not Path(local_file).exists():\n",
    "        with open(\"HF_TOKEN\") as f:\n",
    "            HF_TOKEN = f.read()\n",
    "\n",
    "        display(f\"Downloading {local_file}\")\n",
    "\n",
    "        opener = urllib.request.build_opener()\n",
    "        opener.addheaders = [(\"Authorization\", f\"Bearer {HF_TOKEN}\")]\n",
    "        urllib.request.install_opener(opener)\n",
    "\n",
    "        urllib.request.urlretrieve(required_file, local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfloat16_to_float32(bf16buffer: bytes) -> bytes:\n",
    "    assert len(bf16buffer) % 2 == 0, \"the bfloat16 buffer should have an even number of bytes\"\n",
    "    # bfloat16 are exponent-truncated float32s. just add zeros to convert them to float32\n",
    "    result = bytearray(len(bf16buffer) * 2)\n",
    "    for i in range(0, len(bf16buffer), 2):\n",
    "        # endianness: little-endian\n",
    "        # BF16: 2 bytes : [ B0, B1 ]\n",
    "        # Float32: 4bytes : [0, 0, B0, B1]\n",
    "        result[i * 2 + 2: i * 2 + 4] = bf16buffer[i : i + 2]\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting model.embed_tokens.weight, 16808, 525353384\n",
      "Extracting model.layers.0.input_layernorm.weight, 525353384, 525357480\n",
      "Extracting model.layers.0.mlp.down_proj.weight, 525357480, 558911912\n",
      "Extracting model.layers.0.mlp.gate_proj.weight, 558911912, 592466344\n",
      "Extracting model.layers.0.mlp.up_proj.weight, 592466344, 626020776\n",
      "Extracting model.layers.0.post_attention_layernorm.weight, 626020776, 626024872\n",
      "Extracting model.layers.0.self_attn.k_proj.weight, 626024872, 628122024\n",
      "Extracting model.layers.0.self_attn.o_proj.weight, 628122024, 636510632\n",
      "Extracting model.layers.0.self_attn.q_proj.weight, 636510632, 644899240\n",
      "Extracting model.layers.0.self_attn.v_proj.weight, 644899240, 646996392\n",
      "Extracting model.layers.1.input_layernorm.weight, 646996392, 647000488\n",
      "Extracting model.layers.1.mlp.down_proj.weight, 647000488, 680554920\n",
      "Extracting model.layers.1.mlp.gate_proj.weight, 680554920, 714109352\n",
      "Extracting model.layers.1.mlp.up_proj.weight, 714109352, 747663784\n",
      "Extracting model.layers.1.post_attention_layernorm.weight, 747663784, 747667880\n",
      "Extracting model.layers.1.self_attn.k_proj.weight, 747667880, 749765032\n",
      "Extracting model.layers.1.self_attn.o_proj.weight, 749765032, 758153640\n",
      "Extracting model.layers.1.self_attn.q_proj.weight, 758153640, 766542248\n",
      "Extracting model.layers.1.self_attn.v_proj.weight, 766542248, 768639400\n",
      "Extracting model.layers.10.input_layernorm.weight, 768639400, 768643496\n",
      "Extracting model.layers.10.mlp.down_proj.weight, 768643496, 802197928\n",
      "Extracting model.layers.10.mlp.gate_proj.weight, 802197928, 835752360\n",
      "Extracting model.layers.10.mlp.up_proj.weight, 835752360, 869306792\n",
      "Extracting model.layers.10.post_attention_layernorm.weight, 869306792, 869310888\n",
      "Extracting model.layers.10.self_attn.k_proj.weight, 869310888, 871408040\n",
      "Extracting model.layers.10.self_attn.o_proj.weight, 871408040, 879796648\n",
      "Extracting model.layers.10.self_attn.q_proj.weight, 879796648, 888185256\n",
      "Extracting model.layers.10.self_attn.v_proj.weight, 888185256, 890282408\n",
      "Extracting model.layers.11.input_layernorm.weight, 890282408, 890286504\n",
      "Extracting model.layers.11.mlp.down_proj.weight, 890286504, 923840936\n",
      "Extracting model.layers.11.mlp.gate_proj.weight, 923840936, 957395368\n",
      "Extracting model.layers.11.mlp.up_proj.weight, 957395368, 990949800\n",
      "Extracting model.layers.11.post_attention_layernorm.weight, 990949800, 990953896\n",
      "Extracting model.layers.11.self_attn.k_proj.weight, 990953896, 993051048\n",
      "Extracting model.layers.11.self_attn.o_proj.weight, 993051048, 1001439656\n",
      "Extracting model.layers.11.self_attn.q_proj.weight, 1001439656, 1009828264\n",
      "Extracting model.layers.11.self_attn.v_proj.weight, 1009828264, 1011925416\n",
      "Extracting model.layers.12.input_layernorm.weight, 1011925416, 1011929512\n",
      "Extracting model.layers.12.mlp.down_proj.weight, 1011929512, 1045483944\n",
      "Extracting model.layers.12.mlp.gate_proj.weight, 1045483944, 1079038376\n",
      "Extracting model.layers.12.mlp.up_proj.weight, 1079038376, 1112592808\n",
      "Extracting model.layers.12.post_attention_layernorm.weight, 1112592808, 1112596904\n",
      "Extracting model.layers.12.self_attn.k_proj.weight, 1112596904, 1114694056\n",
      "Extracting model.layers.12.self_attn.o_proj.weight, 1114694056, 1123082664\n",
      "Extracting model.layers.12.self_attn.q_proj.weight, 1123082664, 1131471272\n",
      "Extracting model.layers.12.self_attn.v_proj.weight, 1131471272, 1133568424\n",
      "Extracting model.layers.13.input_layernorm.weight, 1133568424, 1133572520\n",
      "Extracting model.layers.13.mlp.down_proj.weight, 1133572520, 1167126952\n",
      "Extracting model.layers.13.mlp.gate_proj.weight, 1167126952, 1200681384\n",
      "Extracting model.layers.13.mlp.up_proj.weight, 1200681384, 1234235816\n",
      "Extracting model.layers.13.post_attention_layernorm.weight, 1234235816, 1234239912\n",
      "Extracting model.layers.13.self_attn.k_proj.weight, 1234239912, 1236337064\n",
      "Extracting model.layers.13.self_attn.o_proj.weight, 1236337064, 1244725672\n",
      "Extracting model.layers.13.self_attn.q_proj.weight, 1244725672, 1253114280\n",
      "Extracting model.layers.13.self_attn.v_proj.weight, 1253114280, 1255211432\n",
      "Extracting model.layers.14.input_layernorm.weight, 1255211432, 1255215528\n",
      "Extracting model.layers.14.mlp.down_proj.weight, 1255215528, 1288769960\n",
      "Extracting model.layers.14.mlp.gate_proj.weight, 1288769960, 1322324392\n",
      "Extracting model.layers.14.mlp.up_proj.weight, 1322324392, 1355878824\n",
      "Extracting model.layers.14.post_attention_layernorm.weight, 1355878824, 1355882920\n",
      "Extracting model.layers.14.self_attn.k_proj.weight, 1355882920, 1357980072\n",
      "Extracting model.layers.14.self_attn.o_proj.weight, 1357980072, 1366368680\n",
      "Extracting model.layers.14.self_attn.q_proj.weight, 1366368680, 1374757288\n",
      "Extracting model.layers.14.self_attn.v_proj.weight, 1374757288, 1376854440\n",
      "Extracting model.layers.15.input_layernorm.weight, 1376854440, 1376858536\n",
      "Extracting model.layers.15.mlp.down_proj.weight, 1376858536, 1410412968\n",
      "Extracting model.layers.15.mlp.gate_proj.weight, 1410412968, 1443967400\n",
      "Extracting model.layers.15.mlp.up_proj.weight, 1443967400, 1477521832\n",
      "Extracting model.layers.15.post_attention_layernorm.weight, 1477521832, 1477525928\n",
      "Extracting model.layers.15.self_attn.k_proj.weight, 1477525928, 1479623080\n",
      "Extracting model.layers.15.self_attn.o_proj.weight, 1479623080, 1488011688\n",
      "Extracting model.layers.15.self_attn.q_proj.weight, 1488011688, 1496400296\n",
      "Extracting model.layers.15.self_attn.v_proj.weight, 1496400296, 1498497448\n",
      "Extracting model.layers.2.input_layernorm.weight, 1498497448, 1498501544\n",
      "Extracting model.layers.2.mlp.down_proj.weight, 1498501544, 1532055976\n",
      "Extracting model.layers.2.mlp.gate_proj.weight, 1532055976, 1565610408\n",
      "Extracting model.layers.2.mlp.up_proj.weight, 1565610408, 1599164840\n",
      "Extracting model.layers.2.post_attention_layernorm.weight, 1599164840, 1599168936\n",
      "Extracting model.layers.2.self_attn.k_proj.weight, 1599168936, 1601266088\n",
      "Extracting model.layers.2.self_attn.o_proj.weight, 1601266088, 1609654696\n",
      "Extracting model.layers.2.self_attn.q_proj.weight, 1609654696, 1618043304\n",
      "Extracting model.layers.2.self_attn.v_proj.weight, 1618043304, 1620140456\n",
      "Extracting model.layers.3.input_layernorm.weight, 1620140456, 1620144552\n",
      "Extracting model.layers.3.mlp.down_proj.weight, 1620144552, 1653698984\n",
      "Extracting model.layers.3.mlp.gate_proj.weight, 1653698984, 1687253416\n",
      "Extracting model.layers.3.mlp.up_proj.weight, 1687253416, 1720807848\n",
      "Extracting model.layers.3.post_attention_layernorm.weight, 1720807848, 1720811944\n",
      "Extracting model.layers.3.self_attn.k_proj.weight, 1720811944, 1722909096\n",
      "Extracting model.layers.3.self_attn.o_proj.weight, 1722909096, 1731297704\n",
      "Extracting model.layers.3.self_attn.q_proj.weight, 1731297704, 1739686312\n",
      "Extracting model.layers.3.self_attn.v_proj.weight, 1739686312, 1741783464\n",
      "Extracting model.layers.4.input_layernorm.weight, 1741783464, 1741787560\n",
      "Extracting model.layers.4.mlp.down_proj.weight, 1741787560, 1775341992\n",
      "Extracting model.layers.4.mlp.gate_proj.weight, 1775341992, 1808896424\n",
      "Extracting model.layers.4.mlp.up_proj.weight, 1808896424, 1842450856\n",
      "Extracting model.layers.4.post_attention_layernorm.weight, 1842450856, 1842454952\n",
      "Extracting model.layers.4.self_attn.k_proj.weight, 1842454952, 1844552104\n",
      "Extracting model.layers.4.self_attn.o_proj.weight, 1844552104, 1852940712\n",
      "Extracting model.layers.4.self_attn.q_proj.weight, 1852940712, 1861329320\n",
      "Extracting model.layers.4.self_attn.v_proj.weight, 1861329320, 1863426472\n",
      "Extracting model.layers.5.input_layernorm.weight, 1863426472, 1863430568\n",
      "Extracting model.layers.5.mlp.down_proj.weight, 1863430568, 1896985000\n",
      "Extracting model.layers.5.mlp.gate_proj.weight, 1896985000, 1930539432\n",
      "Extracting model.layers.5.mlp.up_proj.weight, 1930539432, 1964093864\n",
      "Extracting model.layers.5.post_attention_layernorm.weight, 1964093864, 1964097960\n",
      "Extracting model.layers.5.self_attn.k_proj.weight, 1964097960, 1966195112\n",
      "Extracting model.layers.5.self_attn.o_proj.weight, 1966195112, 1974583720\n",
      "Extracting model.layers.5.self_attn.q_proj.weight, 1974583720, 1982972328\n",
      "Extracting model.layers.5.self_attn.v_proj.weight, 1982972328, 1985069480\n",
      "Extracting model.layers.6.input_layernorm.weight, 1985069480, 1985073576\n",
      "Extracting model.layers.6.mlp.down_proj.weight, 1985073576, 2018628008\n",
      "Extracting model.layers.6.mlp.gate_proj.weight, 2018628008, 2052182440\n",
      "Extracting model.layers.6.mlp.up_proj.weight, 2052182440, 2085736872\n",
      "Extracting model.layers.6.post_attention_layernorm.weight, 2085736872, 2085740968\n",
      "Extracting model.layers.6.self_attn.k_proj.weight, 2085740968, 2087838120\n",
      "Extracting model.layers.6.self_attn.o_proj.weight, 2087838120, 2096226728\n",
      "Extracting model.layers.6.self_attn.q_proj.weight, 2096226728, 2104615336\n",
      "Extracting model.layers.6.self_attn.v_proj.weight, 2104615336, 2106712488\n",
      "Extracting model.layers.7.input_layernorm.weight, 2106712488, 2106716584\n",
      "Extracting model.layers.7.mlp.down_proj.weight, 2106716584, 2140271016\n",
      "Extracting model.layers.7.mlp.gate_proj.weight, 2140271016, 2173825448\n",
      "Extracting model.layers.7.mlp.up_proj.weight, 2173825448, 2207379880\n",
      "Extracting model.layers.7.post_attention_layernorm.weight, 2207379880, 2207383976\n",
      "Extracting model.layers.7.self_attn.k_proj.weight, 2207383976, 2209481128\n",
      "Extracting model.layers.7.self_attn.o_proj.weight, 2209481128, 2217869736\n",
      "Extracting model.layers.7.self_attn.q_proj.weight, 2217869736, 2226258344\n",
      "Extracting model.layers.7.self_attn.v_proj.weight, 2226258344, 2228355496\n",
      "Extracting model.layers.8.input_layernorm.weight, 2228355496, 2228359592\n",
      "Extracting model.layers.8.mlp.down_proj.weight, 2228359592, 2261914024\n",
      "Extracting model.layers.8.mlp.gate_proj.weight, 2261914024, 2295468456\n",
      "Extracting model.layers.8.mlp.up_proj.weight, 2295468456, 2329022888\n",
      "Extracting model.layers.8.post_attention_layernorm.weight, 2329022888, 2329026984\n",
      "Extracting model.layers.8.self_attn.k_proj.weight, 2329026984, 2331124136\n",
      "Extracting model.layers.8.self_attn.o_proj.weight, 2331124136, 2339512744\n",
      "Extracting model.layers.8.self_attn.q_proj.weight, 2339512744, 2347901352\n",
      "Extracting model.layers.8.self_attn.v_proj.weight, 2347901352, 2349998504\n",
      "Extracting model.layers.9.input_layernorm.weight, 2349998504, 2350002600\n",
      "Extracting model.layers.9.mlp.down_proj.weight, 2350002600, 2383557032\n",
      "Extracting model.layers.9.mlp.gate_proj.weight, 2383557032, 2417111464\n",
      "Extracting model.layers.9.mlp.up_proj.weight, 2417111464, 2450665896\n",
      "Extracting model.layers.9.post_attention_layernorm.weight, 2450665896, 2450669992\n",
      "Extracting model.layers.9.self_attn.k_proj.weight, 2450669992, 2452767144\n",
      "Extracting model.layers.9.self_attn.o_proj.weight, 2452767144, 2461155752\n",
      "Extracting model.layers.9.self_attn.q_proj.weight, 2461155752, 2469544360\n",
      "Extracting model.layers.9.self_attn.v_proj.weight, 2469544360, 2471641512\n",
      "Extracting model.norm.weight, 2471641512, 2471645608\n"
     ]
    }
   ],
   "source": [
    "import mmap\n",
    "import struct\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "model_dir = Path(\"model\")\n",
    "if not model_dir.exists():\n",
    "    model_dir.mkdir(parents=True)\n",
    "    with open(\"model.safetensors\", mode=\"rb\") as file:\n",
    "        # See safetensors file format : https://github.com/huggingface/safetensors\n",
    "        mmaped = mmap.mmap(file.fileno(), 0, prot=mmap.PROT_READ)\n",
    "        header_size = mmaped[:8]\n",
    "        header_size = struct.unpack(\"<Q\", header_size)[0] # little-endinan uint64\n",
    "        header = mmaped[8 : 8 + header_size]\n",
    "        header = json.loads(header)\n",
    "        for tensor_name, tensor_metadata in header.items():\n",
    "            if tensor_name == \"__metadata__\":\n",
    "                continue\n",
    "\n",
    "            start, end = tensor_metadata[\"data_offsets\"]\n",
    "\n",
    "            # header size should must be taken into account:\n",
    "            start += 8 + header_size\n",
    "            end += 8 + header_size\n",
    "\n",
    "            dtype = tensor_metadata[\"dtype\"]\n",
    "            print(f\"Extracting {tensor_name}, {start}, {end}\")\n",
    "            if dtype == \"BF16\":\n",
    "                raw_tensor = bfloat16_to_float32(mmaped[start: end])\n",
    "                with open(model_dir / f\"{tensor_name}.raw\", mode=\"wb\") as file:\n",
    "                    file.write(raw_tensor)\n",
    "        with open(model_dir / \"metadata.json\", mode=\"w\") as file:\n",
    "            json.dump(header, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_raw_model(path: str):\n",
    "    model_dir = Path(path)\n",
    "    model = {}\n",
    "    with open(model_dir / \"metadata.json\") as file:\n",
    "        metadata = json.load(file)\n",
    "    for tensor_name, tensor_metadata in metadata.items():\n",
    "        if tensor_name == \"__metadata__\":\n",
    "            continue\n",
    "        file = open(model_dir / f\"{tensor_name}.raw\", mode=\"rb\")\n",
    "        mmaped = mmap.mmap(file.fileno(), 0, prot=mmap.PROT_READ)\n",
    "        model[tensor_name] = np.frombuffer(mmaped, dtype=np.float32).reshape(tensor_metadata[\"shape\"])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_raw_model(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128256, 2048)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"model.embed_tokens.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model.embed_tokens.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.norm.weight'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = model[\"model.layers.0.input_layernorm.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.transpose().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
