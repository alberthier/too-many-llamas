{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM tokenizer\n",
    "\n",
    "A tokenizer can be seen as a mapping of words to numbers. Well, not exactly words, but sometimes words, sometimes fragments of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: regex in ./.venv/lib/python3.12/site-packages (2024.11.6)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting requests>=2.26.0 (from tiktoken)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.26.0->tiktoken)\n",
      "  Using cached charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.26.0->tiktoken)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.26.0->tiktoken)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.26.0->tiktoken)\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests, tiktoken\n",
      "Successfully installed certifi-2024.12.14 charset-normalizer-3.4.0 idna-3.10 requests-2.32.3 tiktoken-0.8.0 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install regex tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(\"tokenizer.json\").exists():\n",
    "    with open(\"HF_TOKEN\") as f:\n",
    "        HF_TOKEN = f.read()\n",
    "\n",
    "\n",
    "    display(\"Downloading llama 3.2 tokenizer\")\n",
    "\n",
    "    opener = urllib.request.build_opener()\n",
    "    opener.addheaders = [(\"Authorization\", f\"Bearer {HF_TOKEN}\")]\n",
    "    urllib.request.install_opener(opener)\n",
    "\n",
    "    tokenizer_json_path, headers = urllib.request.urlretrieve('https://huggingface.co/meta-llama/Llama-3.2-1B/raw/main/tokenizer.json', \"tokenizer.json\")\n",
    "else:\n",
    "    tokenizer_json_path = \"tokenizer.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenizer data from the JSON file.\n",
    "\n",
    "The split regex will be used to split the text into tokens\n",
    "\n",
    "the vocabulary maps the token strings to their corresponding ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Splitting regex: (?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Vocabulary length: 128000'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "with open(tokenizer_json_path) as f:\n",
    "    tokenizer_data = json.load(f)\n",
    "split = next(filter(lambda t: t[\"type\"] == \"Split\", tokenizer_data[\"pre_tokenizer\"][\"pretokenizers\"]))\n",
    "split_regex = split[\"pattern\"][\"Regex\"]\n",
    "vocab = {k.replace(\"Ġ\", \" \").encode(\"utf-8\"): v for k, v in tokenizer_data[\"model\"][\"vocab\"].items()}\n",
    "\n",
    "display(f\"Splitting regex: {split_regex}\")\n",
    "display(f\"Vocabulary length: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to split a text with this regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inv = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex\n",
    "\n",
    "tok_regex = regex.compile(split_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'m\",\n",
       " 'pretty',\n",
       " 'much',\n",
       " 'fucked',\n",
       " '.',\n",
       " 'That',\n",
       " \"'s\",\n",
       " 'my',\n",
       " 'considered',\n",
       " 'opinion',\n",
       " '.',\n",
       " 'Fucked',\n",
       " '.',\n",
       " 'Six',\n",
       " 'days',\n",
       " 'into',\n",
       " 'what',\n",
       " 'should',\n",
       " 'be',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'greatest',\n",
       " 'two',\n",
       " 'months',\n",
       " 'of',\n",
       " 'my',\n",
       " 'life',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'turned',\n",
       " 'into',\n",
       " 'a',\n",
       " 'nightmare',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I'm pretty much fucked. That's my considered opinion. Fucked. Six days into what should be one of the greatest two months of my life, and it's turned into a nightmare.\"\n",
    "tokens = [t.strip() for t in tok_regex.findall(text)]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe_encode(\n",
    "    mergeable_ranks: dict[bytes, int], input: str, visualise: str | None = \"colour\"\n",
    ") -> list[int]:\n",
    "    parts = [bytes([b]) for b in input]\n",
    "    while True:\n",
    "        # See the intermediate merges play out!\n",
    "        # if visualise:\n",
    "        #     if visualise in [\"colour\", \"color\"]:\n",
    "        #         visualise_tokens(parts)\n",
    "        #     elif visualise == \"simple\":\n",
    "        #         print(parts)\n",
    "\n",
    "        # Iterate over all pairs and find the pair we want to merge the most\n",
    "        min_idx = None\n",
    "        min_rank = None\n",
    "        for i, pair in enumerate(zip(parts[:-1], parts[1:])):\n",
    "            rank = mergeable_ranks.get(pair[0] + pair[1])\n",
    "            if rank is not None and (min_rank is None or rank < min_rank):\n",
    "                min_idx = i\n",
    "                min_rank = rank\n",
    "\n",
    "        # If there were no pairs we could merge, we're done!\n",
    "        if min_rank is None:\n",
    "            break\n",
    "        assert min_idx is not None\n",
    "\n",
    "        # Otherwise, merge that pair and leave the rest unchanged. Then repeat.\n",
    "        parts = parts[:min_idx] + [parts[min_idx] + parts[min_idx + 1]] + parts[min_idx + 2 :]\n",
    "\n",
    "    if visualise:\n",
    "        print()\n",
    "\n",
    "    tokens = [mergeable_ranks[part] for part in parts]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'hello']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15339]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe_encode(vocab, b\"hello\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'hello'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inv[15339]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40]\n",
      "[2846]\n",
      "[34055]\n",
      "[59178]\n",
      "[69, 40458]\n",
      "[13]\n",
      "[4897]\n",
      "[596]\n",
      "[2465]\n",
      "[25742, 291]\n",
      "[454, 37400]\n",
      "[13]\n",
      "[37, 40458]\n",
      "[13]\n",
      "[42560]\n",
      "[14097]\n",
      "[18614]\n",
      "[12840]\n",
      "[5562]\n",
      "[1395]\n",
      "[606]\n",
      "[1073]\n",
      "[1820]\n",
      "[70, 11423]\n",
      "[20375]\n",
      "[50814]\n",
      "[1073]\n",
      "[2465]\n",
      "[14789]\n",
      "[11]\n",
      "[438]\n",
      "[275]\n",
      "[596]\n",
      "[42286]\n",
      "[18614]\n",
      "[64]\n",
      "[9471, 28755]\n",
      "[13]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[40,\n",
       " 2846,\n",
       " 34055,\n",
       " 59178,\n",
       " 69,\n",
       " 40458,\n",
       " 13,\n",
       " 4897,\n",
       " 596,\n",
       " 2465,\n",
       " 25742,\n",
       " 291,\n",
       " 454,\n",
       " 37400,\n",
       " 13,\n",
       " 37,\n",
       " 40458,\n",
       " 13,\n",
       " 42560,\n",
       " 14097,\n",
       " 18614,\n",
       " 12840,\n",
       " 5562,\n",
       " 1395,\n",
       " 606,\n",
       " 1073,\n",
       " 1820,\n",
       " 70,\n",
       " 11423,\n",
       " 20375,\n",
       " 50814,\n",
       " 1073,\n",
       " 2465,\n",
       " 14789,\n",
       " 11,\n",
       " 438,\n",
       " 275,\n",
       " 596,\n",
       " 42286,\n",
       " 18614,\n",
       " 64,\n",
       " 9471,\n",
       " 28755,\n",
       " 13]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded = []\n",
    "for token in tokens:\n",
    "    enc = bpe_encode(vocab, token, None)\n",
    "    print(enc)\n",
    "    encoded.extend(enc)\n",
    "\n",
    "display(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I 'm pretty much f ucked . That 's my consider ed op inion . F ucked . Six days into what should be one of the g reatest two months of my life , and it 's turned into a night mare . \""
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = \"\"\n",
    "\n",
    "for t in encoded:\n",
    "    decoded += vocab_inv[t] + \" \"\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "cl100k_base = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'I'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m encoded \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tok_regex\u001b[38;5;241m.\u001b[39mfindall(text):\n\u001b[0;32m----> 3\u001b[0m     enc \u001b[38;5;241m=\u001b[39m \u001b[43mbpe_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcl100k_base\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mergeable_ranks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(enc)\n\u001b[1;32m      5\u001b[0m     encoded\u001b[38;5;241m.\u001b[39mextend(enc)\n",
      "Cell \u001b[0;32mIn[87], line 33\u001b[0m, in \u001b[0;36mbpe_encode\u001b[0;34m(mergeable_ranks, input, visualise)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualise:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 33\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mmergeable_ranks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpart\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m parts]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[0;31mKeyError\u001b[0m: 'I'"
     ]
    }
   ],
   "source": [
    "\n",
    "encoded = []\n",
    "for token in tok_regex.findall(text):\n",
    "    enc = bpe_encode(cl100k_base._mergeable_ranks, token, None)\n",
    "    print(enc)\n",
    "    encoded.extend(enc)\n",
    "\n",
    "display(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'(?i:[sdmt]|ll|ve|re)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?+\\\\p{L}++|\\\\p{N}{1,3}+| ?[^\\\\s\\\\p{L}\\\\p{N}]++[\\\\r\\\\n]*+|\\\\s++$|\\\\s*[\\\\r\\\\n]|\\\\s+(?!\\\\S)|\\\\s\""
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl100k_base._pat_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\\\r\\\\n\\\\p{L}\\\\p{N}]?\\\\p{L}+|\\\\p{N}{1,3}| ?[^\\\\s\\\\p{L}\\\\p{N}]+[\\\\r\\\\n]*|\\\\s*[\\\\r\\\\n]+|\\\\s+(?!\\\\S)|\\\\s+\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " \"'m\",\n",
       " ' pretty',\n",
       " ' much',\n",
       " ' fucked',\n",
       " '.',\n",
       " ' That',\n",
       " \"'s\",\n",
       " ' my',\n",
       " ' considered',\n",
       " ' opinion',\n",
       " '.',\n",
       " ' Fucked',\n",
       " '.',\n",
       " ' Six',\n",
       " ' days',\n",
       " ' into',\n",
       " ' what',\n",
       " ' should',\n",
       " ' be',\n",
       " ' one',\n",
       " ' of',\n",
       " ' the',\n",
       " ' greatest',\n",
       " ' two',\n",
       " ' months',\n",
       " ' of',\n",
       " ' my',\n",
       " ' life',\n",
       " ',',\n",
       " ' and',\n",
       " ' it',\n",
       " \"'s\",\n",
       " ' turned',\n",
       " ' into',\n",
       " ' a',\n",
       " ' nightmare',\n",
       " '.']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex.compile(cl100k_base._pat_str).findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b' pretty'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" pretty\".encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken._educational as tike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tke = tike.SimpleBytePairEncoding(pat_str=cl100k_base._pat_str, mergeable_ranks=cl100k_base._mergeable_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;5;167mI\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m'\u001b[48;5;179mm\u001b[0m\n",
      "\u001b[48;5;167m'm\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mp\u001b[48;5;185mr\u001b[48;5;77me\u001b[48;5;80mt\u001b[48;5;68mt\u001b[48;5;134my\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mp\u001b[48;5;185mre\u001b[48;5;80mt\u001b[48;5;68mt\u001b[48;5;134my\u001b[0m\n",
      "\u001b[48;5;167m p\u001b[48;5;185mre\u001b[48;5;80mt\u001b[48;5;68mt\u001b[48;5;134my\u001b[0m\n",
      "\u001b[48;5;167m pre\u001b[48;5;80mt\u001b[48;5;68mt\u001b[48;5;134my\u001b[0m\n",
      "\u001b[48;5;167m pre\u001b[48;5;80mt\u001b[48;5;68mty\u001b[0m\n",
      "\u001b[48;5;167m pret\u001b[48;5;68mty\u001b[0m\n",
      "\u001b[48;5;167m pretty\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mm\u001b[48;5;185mu\u001b[48;5;77mc\u001b[48;5;80mh\u001b[0m\n",
      "\u001b[48;5;167m m\u001b[48;5;185mu\u001b[48;5;77mc\u001b[48;5;80mh\u001b[0m\n",
      "\u001b[48;5;167m m\u001b[48;5;185mu\u001b[48;5;77mch\u001b[0m\n",
      "\u001b[48;5;167m m\u001b[48;5;185much\u001b[0m\n",
      "\u001b[48;5;167m much\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mf\u001b[48;5;185mu\u001b[48;5;77mc\u001b[48;5;80mk\u001b[48;5;68me\u001b[48;5;134md\u001b[0m\n",
      "\u001b[48;5;167m f\u001b[48;5;185mu\u001b[48;5;77mc\u001b[48;5;80mk\u001b[48;5;68me\u001b[48;5;134md\u001b[0m\n",
      "\u001b[48;5;167m f\u001b[48;5;185mu\u001b[48;5;77mc\u001b[48;5;80mk\u001b[48;5;68med\u001b[0m\n",
      "\u001b[48;5;167m f\u001b[48;5;185mu\u001b[48;5;77mck\u001b[48;5;68med\u001b[0m\n",
      "\u001b[48;5;167m f\u001b[48;5;185muck\u001b[48;5;68med\u001b[0m\n",
      "\u001b[48;5;167m fuck\u001b[48;5;68med\u001b[0m\n",
      "\u001b[48;5;167m fucked\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m.\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mT\u001b[48;5;185mh\u001b[48;5;77ma\u001b[48;5;80mt\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mT\u001b[48;5;185mh\u001b[48;5;77mat\u001b[0m\n",
      "\u001b[48;5;167m T\u001b[48;5;185mh\u001b[48;5;77mat\u001b[0m\n",
      "\u001b[48;5;167m Th\u001b[48;5;77mat\u001b[0m\n",
      "\u001b[48;5;167m That\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m'\u001b[48;5;179ms\u001b[0m\n",
      "\u001b[48;5;167m's\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mm\u001b[48;5;185my\u001b[0m\n",
      "\u001b[48;5;167m m\u001b[48;5;185my\u001b[0m\n",
      "\u001b[48;5;167m my\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mc\u001b[48;5;185mo\u001b[48;5;77mn\u001b[48;5;80ms\u001b[48;5;68mi\u001b[48;5;134md\u001b[48;5;167me\u001b[48;5;179mr\u001b[48;5;185me\u001b[48;5;77md\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mc\u001b[48;5;185mo\u001b[48;5;77mn\u001b[48;5;80ms\u001b[48;5;68mi\u001b[48;5;134md\u001b[48;5;167mer\u001b[48;5;185me\u001b[48;5;77md\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mc\u001b[48;5;185mon\u001b[48;5;80ms\u001b[48;5;68mi\u001b[48;5;134md\u001b[48;5;167mer\u001b[48;5;185me\u001b[48;5;77md\u001b[0m\n",
      "\u001b[48;5;167m c\u001b[48;5;185mon\u001b[48;5;80ms\u001b[48;5;68mi\u001b[48;5;134md\u001b[48;5;167mer\u001b[48;5;185me\u001b[48;5;77md\u001b[0m\n",
      "\u001b[48;5;167m c\u001b[48;5;185mon\u001b[48;5;80ms\u001b[48;5;68mi\u001b[48;5;134md\u001b[48;5;167mer\u001b[48;5;185med\u001b[0m\n",
      "\u001b[48;5;167m c\u001b[48;5;185mon\u001b[48;5;80ms\u001b[48;5;68mid\u001b[48;5;167mer\u001b[48;5;185med\u001b[0m\n",
      "\u001b[48;5;167m con\u001b[48;5;80ms\u001b[48;5;68mid\u001b[48;5;167mer\u001b[48;5;185med\u001b[0m\n",
      "\u001b[48;5;167m cons\u001b[48;5;68mid\u001b[48;5;167mer\u001b[48;5;185med\u001b[0m\n",
      "\u001b[48;5;167m cons\u001b[48;5;68mider\u001b[48;5;185med\u001b[0m\n",
      "\u001b[48;5;167m consider\u001b[48;5;185med\u001b[0m\n",
      "\u001b[48;5;167m considered\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mo\u001b[48;5;185mp\u001b[48;5;77mi\u001b[48;5;80mn\u001b[48;5;68mi\u001b[48;5;134mo\u001b[48;5;167mn\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mo\u001b[48;5;185mp\u001b[48;5;77min\u001b[48;5;68mi\u001b[48;5;134mo\u001b[48;5;167mn\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mo\u001b[48;5;185mp\u001b[48;5;77min\u001b[48;5;68mi\u001b[48;5;134mon\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mo\u001b[48;5;185mp\u001b[48;5;77min\u001b[48;5;68mion\u001b[0m\n",
      "\u001b[48;5;167m o\u001b[48;5;185mp\u001b[48;5;77min\u001b[48;5;68mion\u001b[0m\n",
      "\u001b[48;5;167m op\u001b[48;5;77min\u001b[48;5;68mion\u001b[0m\n",
      "\u001b[48;5;167m opin\u001b[48;5;68mion\u001b[0m\n",
      "\u001b[48;5;167m opinion\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m.\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mF\u001b[48;5;185mu\u001b[48;5;77mc\u001b[48;5;80mk\u001b[48;5;68me\u001b[48;5;134md\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mF\u001b[48;5;185mu\u001b[48;5;77mc\u001b[48;5;80mk\u001b[48;5;68med\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mF\u001b[48;5;185mu\u001b[48;5;77mck\u001b[48;5;68med\u001b[0m\n",
      "\u001b[48;5;167m F\u001b[48;5;185mu\u001b[48;5;77mck\u001b[48;5;68med\u001b[0m\n",
      "\u001b[48;5;167m F\u001b[48;5;185muck\u001b[48;5;68med\u001b[0m\n",
      "\u001b[48;5;167m Fuck\u001b[48;5;68med\u001b[0m\n",
      "\u001b[48;5;167m Fucked\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m.\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mS\u001b[48;5;185mi\u001b[48;5;77mx\u001b[0m\n",
      "\u001b[48;5;167m S\u001b[48;5;185mi\u001b[48;5;77mx\u001b[0m\n",
      "\u001b[48;5;167m S\u001b[48;5;185mix\u001b[0m\n",
      "\u001b[48;5;167m Six\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179md\u001b[48;5;185ma\u001b[48;5;77my\u001b[48;5;80ms\u001b[0m\n",
      "\u001b[48;5;167m d\u001b[48;5;185ma\u001b[48;5;77my\u001b[48;5;80ms\u001b[0m\n",
      "\u001b[48;5;167m d\u001b[48;5;185may\u001b[48;5;80ms\u001b[0m\n",
      "\u001b[48;5;167m d\u001b[48;5;185mays\u001b[0m\n",
      "\u001b[48;5;167m days\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mi\u001b[48;5;185mn\u001b[48;5;77mt\u001b[48;5;80mo\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179min\u001b[48;5;77mt\u001b[48;5;80mo\u001b[0m\n",
      "\u001b[48;5;167m in\u001b[48;5;77mt\u001b[48;5;80mo\u001b[0m\n",
      "\u001b[48;5;167m int\u001b[48;5;80mo\u001b[0m\n",
      "\u001b[48;5;167m into\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mw\u001b[48;5;185mh\u001b[48;5;77ma\u001b[48;5;80mt\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mw\u001b[48;5;185mh\u001b[48;5;77mat\u001b[0m\n",
      "\u001b[48;5;167m w\u001b[48;5;185mh\u001b[48;5;77mat\u001b[0m\n",
      "\u001b[48;5;167m wh\u001b[48;5;77mat\u001b[0m\n",
      "\u001b[48;5;167m what\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179ms\u001b[48;5;185mh\u001b[48;5;77mo\u001b[48;5;80mu\u001b[48;5;68ml\u001b[48;5;134md\u001b[0m\n",
      "\u001b[48;5;167m s\u001b[48;5;185mh\u001b[48;5;77mo\u001b[48;5;80mu\u001b[48;5;68ml\u001b[48;5;134md\u001b[0m\n",
      "\u001b[48;5;167m s\u001b[48;5;185mh\u001b[48;5;77mou\u001b[48;5;68ml\u001b[48;5;134md\u001b[0m\n",
      "\u001b[48;5;167m s\u001b[48;5;185mh\u001b[48;5;77mou\u001b[48;5;68mld\u001b[0m\n",
      "\u001b[48;5;167m sh\u001b[48;5;77mou\u001b[48;5;68mld\u001b[0m\n",
      "\u001b[48;5;167m sh\u001b[48;5;77mould\u001b[0m\n",
      "\u001b[48;5;167m should\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mb\u001b[48;5;185me\u001b[0m\n",
      "\u001b[48;5;167m b\u001b[48;5;185me\u001b[0m\n",
      "\u001b[48;5;167m be\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mo\u001b[48;5;185mn\u001b[48;5;77me\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mon\u001b[48;5;77me\u001b[0m\n",
      "\u001b[48;5;167m on\u001b[48;5;77me\u001b[0m\n",
      "\u001b[48;5;167m one\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mo\u001b[48;5;185mf\u001b[0m\n",
      "\u001b[48;5;167m o\u001b[48;5;185mf\u001b[0m\n",
      "\u001b[48;5;167m of\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mt\u001b[48;5;185mh\u001b[48;5;77me\u001b[0m\n",
      "\u001b[48;5;167m t\u001b[48;5;185mh\u001b[48;5;77me\u001b[0m\n",
      "\u001b[48;5;167m th\u001b[48;5;77me\u001b[0m\n",
      "\u001b[48;5;167m the\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mg\u001b[48;5;185mr\u001b[48;5;77me\u001b[48;5;80ma\u001b[48;5;68mt\u001b[48;5;134me\u001b[48;5;167ms\u001b[48;5;179mt\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mg\u001b[48;5;185mre\u001b[48;5;80ma\u001b[48;5;68mt\u001b[48;5;134me\u001b[48;5;167ms\u001b[48;5;179mt\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mg\u001b[48;5;185mre\u001b[48;5;80mat\u001b[48;5;134me\u001b[48;5;167ms\u001b[48;5;179mt\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mg\u001b[48;5;185mre\u001b[48;5;80mat\u001b[48;5;134me\u001b[48;5;167mst\u001b[0m\n",
      "\u001b[48;5;167m g\u001b[48;5;185mre\u001b[48;5;80mat\u001b[48;5;134me\u001b[48;5;167mst\u001b[0m\n",
      "\u001b[48;5;167m g\u001b[48;5;185mre\u001b[48;5;80mate\u001b[48;5;167mst\u001b[0m\n",
      "\u001b[48;5;167m g\u001b[48;5;185mreate\u001b[48;5;167mst\u001b[0m\n",
      "\u001b[48;5;167m g\u001b[48;5;185mreatest\u001b[0m\n",
      "\u001b[48;5;167m greatest\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mt\u001b[48;5;185mw\u001b[48;5;77mo\u001b[0m\n",
      "\u001b[48;5;167m t\u001b[48;5;185mw\u001b[48;5;77mo\u001b[0m\n",
      "\u001b[48;5;167m t\u001b[48;5;185mwo\u001b[0m\n",
      "\u001b[48;5;167m two\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mm\u001b[48;5;185mo\u001b[48;5;77mn\u001b[48;5;80mt\u001b[48;5;68mh\u001b[48;5;134ms\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mm\u001b[48;5;185mon\u001b[48;5;80mt\u001b[48;5;68mh\u001b[48;5;134ms\u001b[0m\n",
      "\u001b[48;5;167m m\u001b[48;5;185mon\u001b[48;5;80mt\u001b[48;5;68mh\u001b[48;5;134ms\u001b[0m\n",
      "\u001b[48;5;167m m\u001b[48;5;185mon\u001b[48;5;80mth\u001b[48;5;134ms\u001b[0m\n",
      "\u001b[48;5;167m mon\u001b[48;5;80mth\u001b[48;5;134ms\u001b[0m\n",
      "\u001b[48;5;167m month\u001b[48;5;134ms\u001b[0m\n",
      "\u001b[48;5;167m months\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mo\u001b[48;5;185mf\u001b[0m\n",
      "\u001b[48;5;167m o\u001b[48;5;185mf\u001b[0m\n",
      "\u001b[48;5;167m of\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mm\u001b[48;5;185my\u001b[0m\n",
      "\u001b[48;5;167m m\u001b[48;5;185my\u001b[0m\n",
      "\u001b[48;5;167m my\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179ml\u001b[48;5;185mi\u001b[48;5;77mf\u001b[48;5;80me\u001b[0m\n",
      "\u001b[48;5;167m l\u001b[48;5;185mi\u001b[48;5;77mf\u001b[48;5;80me\u001b[0m\n",
      "\u001b[48;5;167m l\u001b[48;5;185mif\u001b[48;5;80me\u001b[0m\n",
      "\u001b[48;5;167m l\u001b[48;5;185mife\u001b[0m\n",
      "\u001b[48;5;167m life\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m,\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179ma\u001b[48;5;185mn\u001b[48;5;77md\u001b[0m\n",
      "\u001b[48;5;167m a\u001b[48;5;185mn\u001b[48;5;77md\u001b[0m\n",
      "\u001b[48;5;167m a\u001b[48;5;185mnd\u001b[0m\n",
      "\u001b[48;5;167m and\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mi\u001b[48;5;185mt\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mit\u001b[0m\n",
      "\u001b[48;5;167m it\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m'\u001b[48;5;179ms\u001b[0m\n",
      "\u001b[48;5;167m's\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mt\u001b[48;5;185mu\u001b[48;5;77mr\u001b[48;5;80mn\u001b[48;5;68me\u001b[48;5;134md\u001b[0m\n",
      "\u001b[48;5;167m t\u001b[48;5;185mu\u001b[48;5;77mr\u001b[48;5;80mn\u001b[48;5;68me\u001b[48;5;134md\u001b[0m\n",
      "\u001b[48;5;167m t\u001b[48;5;185mu\u001b[48;5;77mr\u001b[48;5;80mn\u001b[48;5;68med\u001b[0m\n",
      "\u001b[48;5;167m t\u001b[48;5;185mur\u001b[48;5;80mn\u001b[48;5;68med\u001b[0m\n",
      "\u001b[48;5;167m t\u001b[48;5;185murn\u001b[48;5;68med\u001b[0m\n",
      "\u001b[48;5;167m turn\u001b[48;5;68med\u001b[0m\n",
      "\u001b[48;5;167m turned\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mi\u001b[48;5;185mn\u001b[48;5;77mt\u001b[48;5;80mo\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179min\u001b[48;5;77mt\u001b[48;5;80mo\u001b[0m\n",
      "\u001b[48;5;167m in\u001b[48;5;77mt\u001b[48;5;80mo\u001b[0m\n",
      "\u001b[48;5;167m int\u001b[48;5;80mo\u001b[0m\n",
      "\u001b[48;5;167m into\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179ma\u001b[0m\n",
      "\u001b[48;5;167m a\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mn\u001b[48;5;185mi\u001b[48;5;77mg\u001b[48;5;80mh\u001b[48;5;68mt\u001b[48;5;134mm\u001b[48;5;167ma\u001b[48;5;179mr\u001b[48;5;185me\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mn\u001b[48;5;185mi\u001b[48;5;77mg\u001b[48;5;80mh\u001b[48;5;68mt\u001b[48;5;134mm\u001b[48;5;167ma\u001b[48;5;179mre\u001b[0m\n",
      "\u001b[48;5;167m n\u001b[48;5;185mi\u001b[48;5;77mg\u001b[48;5;80mh\u001b[48;5;68mt\u001b[48;5;134mm\u001b[48;5;167ma\u001b[48;5;179mre\u001b[0m\n",
      "\u001b[48;5;167m n\u001b[48;5;185mig\u001b[48;5;80mh\u001b[48;5;68mt\u001b[48;5;134mm\u001b[48;5;167ma\u001b[48;5;179mre\u001b[0m\n",
      "\u001b[48;5;167m n\u001b[48;5;185mig\u001b[48;5;80mht\u001b[48;5;134mm\u001b[48;5;167ma\u001b[48;5;179mre\u001b[0m\n",
      "\u001b[48;5;167m n\u001b[48;5;185might\u001b[48;5;134mm\u001b[48;5;167ma\u001b[48;5;179mre\u001b[0m\n",
      "\u001b[48;5;167m n\u001b[48;5;185might\u001b[48;5;134mm\u001b[48;5;167mare\u001b[0m\n",
      "\u001b[48;5;167m night\u001b[48;5;134mm\u001b[48;5;167mare\u001b[0m\n",
      "\u001b[48;5;167m night\u001b[48;5;134mmare\u001b[0m\n",
      "\u001b[48;5;167m nightmare\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[40,\n",
       " 2846,\n",
       " 5128,\n",
       " 1790,\n",
       " 28252,\n",
       " 13,\n",
       " 3011,\n",
       " 596,\n",
       " 856,\n",
       " 6646,\n",
       " 9647,\n",
       " 13,\n",
       " 64662,\n",
       " 13,\n",
       " 19198,\n",
       " 2919,\n",
       " 1139,\n",
       " 1148,\n",
       " 1288,\n",
       " 387,\n",
       " 832,\n",
       " 315,\n",
       " 279,\n",
       " 12474,\n",
       " 1403,\n",
       " 4038,\n",
       " 315,\n",
       " 856,\n",
       " 2324,\n",
       " 11,\n",
       " 323,\n",
       " 433,\n",
       " 596,\n",
       " 6656,\n",
       " 1139,\n",
       " 264,\n",
       " 38911,\n",
       " 13]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tke.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40,\n",
       " 2846,\n",
       " 5128,\n",
       " 1790,\n",
       " 28252,\n",
       " 13,\n",
       " 3011,\n",
       " 596,\n",
       " 856,\n",
       " 6646,\n",
       " 9647,\n",
       " 13,\n",
       " 64662,\n",
       " 13,\n",
       " 19198,\n",
       " 2919,\n",
       " 1139,\n",
       " 1148,\n",
       " 1288,\n",
       " 387,\n",
       " 832,\n",
       " 315,\n",
       " 279,\n",
       " 12474,\n",
       " 1403,\n",
       " 4038,\n",
       " 315,\n",
       " 856,\n",
       " 2324,\n",
       " 11,\n",
       " 323,\n",
       " 433,\n",
       " 596,\n",
       " 6656,\n",
       " 1139,\n",
       " 264,\n",
       " 38911,\n",
       " 13]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded = []\n",
    "for token in tok_regex.findall(text):\n",
    "    enc = tike.bpe_encode(cl100k_base._mergeable_ranks, bytes(token, \"utf-8\"), None)\n",
    "    encoded.extend(enc)\n",
    "\n",
    "display(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm pretty much fucked. That's my considered opinion. Fucked. Six days into what should be one of the greatest two months of my life, and it's turned into a nightmare.\""
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tke.decode(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown encoding tokenizer.json.\nPlugins found: ['tiktoken_ext.openai_public']\ntiktoken version: 0.8.0 (are you on latest?)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtike\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSimpleBytePairEncoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizer.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/too-many-llama/1-ipynb-llama/.venv/lib/python3.12/site-packages/tiktoken/_educational.py:77\u001b[0m, in \u001b[0;36mSimpleBytePairEncoding.from_tiktoken\u001b[0;34m(encoding)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_tiktoken\u001b[39m(encoding):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoding, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 77\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m \u001b[43mtiktoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SimpleBytePairEncoding(\n\u001b[1;32m     79\u001b[0m         pat_str\u001b[38;5;241m=\u001b[39mencoding\u001b[38;5;241m.\u001b[39m_pat_str, mergeable_ranks\u001b[38;5;241m=\u001b[39mencoding\u001b[38;5;241m.\u001b[39m_mergeable_ranks\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/too-many-llama/1-ipynb-llama/.venv/lib/python3.12/site-packages/tiktoken/registry.py:79\u001b[0m, in \u001b[0;36mget_encoding\u001b[0;34m(encoding_name)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m ENCODING_CONSTRUCTORS \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoding_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ENCODING_CONSTRUCTORS:\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown encoding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoding_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlugins found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_available_plugin_modules()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtiktoken version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtiktoken\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (are you on latest?)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     85\u001b[0m constructor \u001b[38;5;241m=\u001b[39m ENCODING_CONSTRUCTORS[encoding_name]\n\u001b[1;32m     86\u001b[0m enc \u001b[38;5;241m=\u001b[39m Encoding(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconstructor())\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown encoding tokenizer.json.\nPlugins found: ['tiktoken_ext.openai_public']\ntiktoken version: 0.8.0 (are you on latest?)"
     ]
    }
   ],
   "source": [
    "tike.SimpleBytePairEncoding.from_tiktoken(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_bytes = { k.encode(\"utf_8\"): v for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2  =tike.SimpleBytePairEncoding(pat_str=split_regex, mergeable_ranks=vocab_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;5;167mI\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m'\u001b[48;5;179mm\u001b[0m\n",
      "\u001b[48;5;167m'm\u001b[0m\n",
      "\n",
      "\u001b[48;5;167m \u001b[48;5;179mp\u001b[48;5;185mr\u001b[48;5;77me\u001b[48;5;80mt\u001b[48;5;68mt\u001b[48;5;134my\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mp\u001b[48;5;185mre\u001b[48;5;80mt\u001b[48;5;68mt\u001b[48;5;134my\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mpre\u001b[48;5;80mt\u001b[48;5;68mt\u001b[48;5;134my\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mpre\u001b[48;5;80mt\u001b[48;5;68mty\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mpret\u001b[48;5;68mty\u001b[0m\n",
      "\u001b[48;5;167m \u001b[48;5;179mpretty\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "b' '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m enc2  \u001b[38;5;241m=\u001b[39m\u001b[43mt2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/too-many-llama/1-ipynb-llama/.venv/lib/python3.12/site-packages/tiktoken/_educational.py:35\u001b[0m, in \u001b[0;36mSimpleBytePairEncoding.encode\u001b[0;34m(self, text, visualise)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Turn each word into tokens, using the byte pair encoding algorithm\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     word_bytes \u001b[38;5;241m=\u001b[39m word\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m     word_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mbpe_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmergeable_ranks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_bytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     tokens\u001b[38;5;241m.\u001b[39mextend(word_tokens)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[0;32m~/Projects/too-many-llama/1-ipynb-llama/.venv/lib/python3.12/site-packages/tiktoken/_educational.py:115\u001b[0m, in \u001b[0;36mbpe_encode\u001b[0;34m(mergeable_ranks, input, visualise)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualise:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m--> 115\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [\u001b[43mmergeable_ranks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpart\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m parts]\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[0;31mKeyError\u001b[0m: b' '"
     ]
    }
   ],
   "source": [
    "enc2  =t2.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "blobfile is not installed. Please install it by running `pip install blobfile`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Projects/too-many-llama/1-ipynb-llama/.venv/lib/python3.12/site-packages/tiktoken/load.py:16\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(blobpath)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mblobfile\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'blobfile'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtiktoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizer.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/too-many-llama/1-ipynb-llama/.venv/lib/python3.12/site-packages/tiktoken/load.py:144\u001b[0m, in \u001b[0;36mload_tiktoken_bpe\u001b[0;34m(tiktoken_bpe_file, expected_hash)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[38;5;28mstr\u001b[39m, expected_hash: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# NB: do not add caching to this function\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     contents \u001b[38;5;241m=\u001b[39m \u001b[43mread_file_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_bpe_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    146\u001b[0m         base64\u001b[38;5;241m.\u001b[39mb64decode(token): \u001b[38;5;28mint\u001b[39m(rank)\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m token, rank \u001b[38;5;129;01min\u001b[39;00m (line\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m contents\u001b[38;5;241m.\u001b[39msplitlines() \u001b[38;5;28;01mif\u001b[39;00m line)\n\u001b[1;32m    148\u001b[0m     }\n",
      "File \u001b[0;32m~/Projects/too-many-llama/1-ipynb-llama/.venv/lib/python3.12/site-packages/tiktoken/load.py:63\u001b[0m, in \u001b[0;36mread_file_cached\u001b[0;34m(blobpath, expected_hash)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m contents \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblobpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_hash \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_hash(contents, expected_hash):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHash mismatch for data downloaded from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblobpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_hash\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis may indicate a corrupted download. Please try again.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/too-many-llama/1-ipynb-llama/.venv/lib/python3.12/site-packages/tiktoken/load.py:18\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(blobpath)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mblobfile\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblobfile is not installed. Please install it by running `pip install blobfile`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m blobfile\u001b[38;5;241m.\u001b[39mBlobFile(blobpath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mImportError\u001b[0m: blobfile is not installed. Please install it by running `pip install blobfile`."
     ]
    }
   ],
   "source": [
    "import tiktoken.load\n",
    "\n",
    "\n",
    "tiktoken.load.load_tiktoken_bpe(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
